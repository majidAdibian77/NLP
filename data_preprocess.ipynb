{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data_preprocess.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1mXmECwSNuNhLsI5OyrfJOdm2peZWFPn6","authorship_tag":"ABX9TyMnl8Jb2aFxZ2kawKuTZp3J"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gzlSlDverXqw"},"source":["import libraries and predefined main parameters"]},{"cell_type":"code","metadata":{"id":"J3yP_E5sn3hY","executionInfo":{"status":"ok","timestamp":1616099649488,"user_tz":-210,"elapsed":1474,"user":{"displayName":"Majid A","photoUrl":"","userId":"00481875298271315487"}}},"source":["from sklearn.model_selection import train_test_split\n","import numpy as np\n","import pickle\n","\n","characters = ['ض', 'ص', 'ث', 'ق', 'ف', 'غ', 'ع', 'ه', 'خ', 'ح', 'ج', 'چ', 'پ', 'ش', 'س', 'ی', 'ب', 'ل', 'أ' ,'ا', 'آ', 'ت',\n","              'ن', 'م', 'ک', 'گ', 'ظ', 'ط', 'ز', 'ر', 'ژ', 'ذ', 'د', 'ئ', 'ء', 'و', 'إ', 'ؤ', 'ي', 'ة', '۱', '۲', '۳', '۴', '۵',\n","              '۶', '۷', '۸', '۹', '۰', ' ']\n","persian_numbers = ['\\u06F0', '\\u06F1', '\\u06F2', '\\u06F3', '\\u06F4', '\\u06F5', '\\u06F6', '\\u06F7', '\\u06F8', '\\u06F9']\n","\n","number_of_previous_words = 4\n","current_path = 'drive/My Drive/Colab Notebooks/Next word prediction/for_github/'"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4BtmWl6krqXI"},"source":["### Clear input text\n","Check every characters of text and if it is not in predefined our characher list, remove it."]},{"cell_type":"code","metadata":{"id":"ehE4Nyc4mDil","executionInfo":{"status":"ok","timestamp":1616099651749,"user_tz":-210,"elapsed":1153,"user":{"displayName":"Majid A","photoUrl":"","userId":"00481875298271315487"}}},"source":["def replace_arabic_chars(text):\n","    text = text.replace('\\u0660', '\\u06F0').replace('\\u0661', '\\u06F1').replace('\\u0662', '\\u06F2').replace('\\u0663', '\\u06F3').replace('\\u0664', '\\u06F4').replace('\\u0665', '۵\\u06F5').replace('\\u0666', '\\u06F6').replace('\\u0667', '\\u06F7').replace('\\u0668', '\\u06F8').replace('\\u0669', '\\u06F9')  # arabic number to persian number\n","    text = text.replace('\\u0643', '\\u06A9').replace('u\\0649', '\\u06CC').replace('\\u064A', '\\u06CC').replace('\\u06D5', '\\u0647')  # \"ك\" to \"ک\", \"ى\" to \"ی\", \"ي\" to \"ی\" , \"ە\" to \"ه\"\n","    return text\n","\n","def replace_english_chars(text):\n","    text = text.replace('0', '۰').replace('1', '۱').replace('2', '۲').replace('3', '۳').replace('4', '۴').replace('5', '۵').replace('6', '۶').replace('7', '۷').replace('8', '۸').replace('9', '۹')  # non-breaking space\n","    text = text.replace(';', '؛').replace('?', '؟').replace(',', '،')\n","    return text\n","    \n","def replace_other_chars(text):\n","    text = text.replace('\\n', '').replace('\\u200c', ' ').replace('\\xa0', ' ')  # non-breaking space\n","    return text\n","\n","def clean_text(text):\n","    new_chars = set(text) - set(characters)\n","    has_new_char = False\n","    for char in new_chars:\n","        text = text.replace(char, '')\n","        has_new_char = True\n","    return text, has_new_char"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c-PZiZ16sSSQ"},"source":["### load data\n","Load data from text file and in every lines of it clear text and extraxt any ngram that we define in parameters section."]},{"cell_type":"code","metadata":{"id":"mgKFUi6urWm5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616099656535,"user_tz":-210,"elapsed":3578,"user":{"displayName":"Majid A","photoUrl":"","userId":"00481875298271315487"}},"outputId":"ad7fe009-5286-4d51-a262-abce358cad66"},"source":["def read_data(path, line_limit):\n","    f = open(path, 'r', encoding='UTF-8')\n","    lines = f.readlines()[:line_limit]\n","    previous_words = []\n","    next_words = []\n","    unique_words = set()\n","    for line in lines:\n","        line = replace_arabic_chars(line)\n","        line = replace_english_chars(line)\n","        line = replace_other_chars(line)\n","        line, _ = clean_text(line)\n","        words = line.split()\n","        if len(words) < number_of_previous_words+1:\n","            continue\n","        for i in range(len(words)-number_of_previous_words):\n","            previous_words.append(words[i: i+number_of_previous_words])\n","            next_words.append(words[i+number_of_previous_words])\n","            unique_words.update(set(words))\n","    return previous_words, next_words, list(unique_words)\n","\n","# Exmaple of results of \"read_data\" function\n","previous_worsds, next_words, unique_words = read_data(current_path + 'data/training_set.txt', 100)\n","print(previous_worsds[:5])\n","print(next_words[0])\n","print(unique_words[:10])\n","print(len(unique_words))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[['بزرگترین', 'واحد', 'تولید', 'پارازایلین'], ['واحد', 'تولید', 'پارازایلین', 'خاورمیانه'], ['تولید', 'پارازایلین', 'خاورمیانه', 'به'], ['پارازایلین', 'خاورمیانه', 'به', 'زودی'], ['خاورمیانه', 'به', 'زودی', 'افتتاح']]\n","خاورمیانه\n","['نقالی', 'سپه', 'مستحق', 'وارد', 'یابند', 'هایده', 'نگاه', 'اصلاحی', 'توپ', 'هندو']\n","6218\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q62fzIMBzcol"},"source":["### word indexing\n","Create tow dictionary for unique word to index of it and index of unique word to unique word.\n","\"PAD\" is for padding matrix for littele sentences.\n"]},{"cell_type":"code","metadata":{"id":"B46n8kEQtp25","executionInfo":{"status":"ok","timestamp":1616099665435,"user_tz":-210,"elapsed":1309,"user":{"displayName":"Majid A","photoUrl":"","userId":"00481875298271315487"}}},"source":["def word_indexing(unique_words):\n","    word2index = dict((c, i+1) for i, c in enumerate(unique_words))\n","    word2index['PAD'] = 0\n","    index2word = dict((i+1, c) for i, c in enumerate(unique_words))\n","    index2word[0] = 'PAD'\n","    return word2index, index2word\n","\n","word2index, index2word = word_indexing(unique_words)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LwxSeg_v7xU0"},"source":["### train and test data\n","Split previous_worsds and next_words to train and test data and lebels"]},{"cell_type":"code","metadata":{"id":"QZlCi_CC4HLM","executionInfo":{"status":"ok","timestamp":1616099667036,"user_tz":-210,"elapsed":766,"user":{"displayName":"Majid A","photoUrl":"","userId":"00481875298271315487"}}},"source":["def create_train_test_data(previous_worsds, next_words):\n","    data_train, data_test, labels_train, labels_test = train_test_split(previous_worsds, next_words, test_size=0.05)\n","    return data_train, data_test, labels_train, labels_test\n","\n","data_train, data_test, labels_train, labels_test = create_train_test_data(previous_worsds, next_words)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m35RiS0U8GqH"},"source":["### create matrices\n","Use numpy to create numerical matrices of data and labels for train and test.\n","\n","Save These matrices and main parameters as objects "]},{"cell_type":"code","metadata":{"id":"TWGCfg9NnUV7","executionInfo":{"status":"ok","timestamp":1616099674376,"user_tz":-210,"elapsed":5189,"user":{"displayName":"Majid A","photoUrl":"","userId":"00481875298271315487"}}},"source":["def save_data(data, path):\n","    f = open(current_path + path, 'wb')  \n","    pickle.dump(data, f)\n","    f.close()\n","\n","def create_matrices(previous_worsds, next_words, word2index, data_path, label_path):\n","    data = np.zeros((len(previous_worsds), number_of_previous_words), dtype=int)\n","    labels = np.zeros((len(next_words), 1), dtype=int)\n","    for i, words_list in enumerate(previous_worsds):\n","        for j, word in enumerate(words_list):\n","            data[i, j] =  word2index[word]\n","        labels[i] = word2index[next_words[i]]\n","\n","    save_data(data, data_path)\n","    save_data(labels, label_path)\n","\n","create_matrices(data_train, labels_train, word2index, 'data/data_train.p', 'data/labels_train.p')\n","create_matrices(data_test, labels_test, word2index, 'data/data_test.p', 'data/labels_test.p')\n","parameters = {'number_of_previous_words': number_of_previous_words, 'persian_numbers': persian_numbers, 'characters': characters}\n","save_data(parameters, 'data/parameters.p')\n","save_data(word2index, 'data/word2index.p')\n","save_data(index2word, 'data/index2word.p')"],"execution_count":14,"outputs":[]}]}